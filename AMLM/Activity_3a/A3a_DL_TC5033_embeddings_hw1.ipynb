{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6142099",
   "metadata": {},
   "source": [
    "## TC 5033\n",
    "### Word Embeddings\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Activity 3a: Exploring Word Embeddings with GloVe and Numpy\n",
    "<br>\n",
    "\n",
    "- Objective:\n",
    "    - To understand the concept of word embeddings and their significance in Natural Language Processing.\n",
    "    - To learn how to manipulate and visualize high-dimensional data using dimensionality reduction techniques like PCA and t-SNE.\n",
    "    - To gain hands-on experience in implementing word similarity and analogies using GloVe embeddings and Numpy.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Instructions:\n",
    "    - Download GloVe pre-trained vectors from the provided link in Canvas, the official public project: \n",
    "    Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation\n",
    "    https://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "    - Create a dictorionay of the embeddings so that you carry out fast look ups. Save that dictionary e.g. as a serialized file for faster loading in future uses.\n",
    "    \n",
    "    - PCA and t-SNE Visualization: After loading the GloVe embeddings, use Numpy and Sklearn to perform PCA and t-SNE to reduce the dimensionality of the embeddings and visualize them in a 2D or 3D space.\n",
    "\n",
    "    - Word Similarity: Implement a function that takes a word as input and returns the 'n' most similar words based on their embeddings. You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
    "\n",
    "    - Word Analogies: Implement a function to solve analogies between words. For example, \"man is to king as woman is to ____\". You should use Numpy to implement this function, using libraries that already implement this function (e.g. Gensim) will result in zero points.\n",
    "\n",
    "    - Submission: This activity is to be submitted in teams of 3 or 4. Only one person should submit the final work, with the full names of all team members included in a markdown cell at the beginning of the notebook.\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Evaluation Criteria:\n",
    "\n",
    "    - Code Quality (40%): Your code should be well-organized, clearly commented, and easy to follow. Use also markdown cells for clarity.\n",
    "    \n",
    "   - Functionality (60%): All functions should work as intended, without errors.\n",
    "       - Visualization of PCA and t-SNE (10% each for a total of 20%)\n",
    "       - Similarity function (20%)\n",
    "       - Analogy function (20%)\n",
    "|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329dd09a",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af04b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pickle\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712cf42",
   "metadata": {},
   "source": [
    "#### Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3c9d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.200d.txt'\n",
    "PATH = '/media/pepe/DataUbuntu/Databases/glove_embeddings/glove.6B.50d.txt'\n",
    "emb_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a1eae6",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create dictionary with embeddings\n",
    "def create_emb_dictionary(path):\n",
    "    dict_glove={}\n",
    "    with open(path, 'r', encoding='utf8') as archivo:\n",
    "            for linea in archivo:\n",
    "                a,*b=linea.split()\n",
    "                dict_glove[a]=b\n",
    "    return dict_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a0e5e",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "embeddings_dict = create_emb_dictionary(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01b760",
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# Serialize\n",
    "#with open('embeddings_dict_50D.pkl', 'wb') as f:\n",
    "#    pickle.dump(embeddings_dict, f)\n",
    "\n",
    "# Deserialize\n",
    "with open('embeddings_dict_50D.pkl', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ab9aa",
   "metadata": {},
   "source": [
    "#### See some embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0991a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some\n",
    "def show_n_first_words(path, n_words):\n",
    "        with open(path, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                print(line.split(), len(line.split()[1:]))\n",
    "                if i>=n_words: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16259ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_n_first_words(PATH, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff828123",
   "metadata": {},
   "source": [
    "### Plot some embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3ffaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(emb_path, words2show, emb_dim, embeddings_dict, func = PCA):\n",
    "    X=[]\n",
    "    for word in words:\n",
    "        X.append(embeddings_dict[word])\n",
    "        \n",
    "    transform=func(n_components=2)\n",
    "    X_reduced=transform.fit_transform(np.asarray(X))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        x,y = X_reduced[i]\n",
    "        plt.scatter(x,y)\n",
    "        plt.annotate(word,(x,y), fontsize=8)\n",
    "        \n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.title('Word Embeddings Visualization')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1b120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words= ['burger', 'tortilla', 'bread', 'pizza', 'beef', 'steak', 'fries', 'chips', \n",
    "            'argentina', 'mexico', 'spain', 'usa', 'france', 'italy', 'greece', 'china',\n",
    "            'water', 'beer', 'tequila', 'wine', 'whisky', 'brandy', 'vodka', 'coffee', 'tea',\n",
    "            'apple', 'banana', 'orange', 'lemon', 'grapefruit', 'grape', 'strawberry', 'raspberry',\n",
    "            'school', 'work', 'university', 'highschool']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d33789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "plot_embeddings(PATH, words, emb_dim, embeddings_dict, PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE dimensionality reduction for visualization\n",
    "embeddings = plot_embeddings(PATH, words, emb_dim, embeddings_dict, TSNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8211b7f1",
   "metadata": {},
   "source": [
    "### Let us compute analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003ce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogy\n",
    "def analogy(word1, word2, word3, embeddings_dict):\n",
    "    emb_a=np.asarray(embeddings_dict[word1], dtype=np.float32)\n",
    "    emb_b=np.asarray(embeddings_dict[word2], dtype=np.float32)\n",
    "    emb_c=np.asarray(embeddings_dict[word3], dtype=np.float32)\n",
    "\n",
    "    words = embeddings_dict.keys()\n",
    "    max_cosine_sim = -999              \n",
    "    best_word = None\n",
    "\n",
    "    for w in words:        \n",
    "        # ignore input words\n",
    "        if w in [word1, word2, word3] :\n",
    "            continue\n",
    "\n",
    "        # Compute cosine similarity between the vectors u and v\n",
    "        #u:(e_b - e_a) \n",
    "        #v:((w's vector representation) - e_c)\n",
    "        \n",
    "\n",
    "        u=emb_b - emb_a\n",
    "        v=np.asarray(embeddings_dict[w], dtype=np.float32) - emb_c\n",
    "        \n",
    "        # find the dot product between u and v \n",
    "        dot = np.dot(u,v)\n",
    "        # find the L2 norm of u \n",
    "        norm_u = np.sqrt(np.sum(u**2))\n",
    "        # Compute the L2 norm of v\n",
    "        norm_v = np.sqrt(np.sum(v**2))\n",
    "        # Compute the cosine similarity\n",
    "        cosine_sim = dot/(norm_u)/norm_v\n",
    "        \n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            # update word_d\n",
    "            best_word = w\n",
    "        \n",
    "    return best_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c02ec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy('man', 'king', 'woman', embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50e0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most similar\n",
    "def find_most_similar(word, embeddings_dict, top_n=10):\n",
    "    word_tarjet=word\n",
    "    similar={}\n",
    "    u=np.asarray(embeddings_dict[word_tarjet], dtype=np.float32)\n",
    "    \n",
    "    for word in embeddings_dict.keys():     \n",
    "        \n",
    "        v=np.asarray(embeddings_dict[word], dtype=np.float32)\n",
    "        \n",
    "        # find the dot product between u and v \n",
    "        dot = np.dot(u,v)\n",
    "        # find the L2 norm of u \n",
    "        norm_u = np.linalg.norm(u)\n",
    "        # Compute the L2 norm of v\n",
    "        norm_v = np.linalg.norm(v)\n",
    "        # Compute the cosine similarity\n",
    "        cosine_sim = dot/(norm_u * norm_v)\n",
    "        \n",
    "        similar[word] = cosine_sim\n",
    "        \n",
    "    sorted_similar = sorted(similar.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_similar[1:top_n+1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7946d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar = find_most_similar('mexico', embeddings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6e0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, w in enumerate(most_similar, 1):\n",
    "    print(f'{i} ---> {w[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09875cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985bfda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f33c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e86fa7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e1838b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1a9fad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
