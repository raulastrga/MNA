{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e38b863",
   "metadata": {},
   "source": [
    "# Team members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b5edb",
   "metadata": {},
   "source": [
    "| Id        | Student                                 |\n",
    "|-----------|-----------------------------------------|\n",
    "| A01795654 | Raul Astorga Castro                     |\n",
    "| A01795579 | Edson Misael Astorga Castro             |\n",
    "| A01373679 | Luis Miguel Balderas González de Burgos |\n",
    "| A01730466 | Sinaí Avalos Rivera                     |\n",
    "| A01410682 | Carlos Miguel Arvizu Durán              |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
   "metadata": {},
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Transformers\n",
    "\n",
    "## Activity 4: Implementing a Translator\n",
    "\n",
    "- Objective\n",
    "\n",
    "To understand the Transformer Architecture by Implementing a translator.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n",
    "\n",
    "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
    "  \n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Traning a translator\n",
    "    - Translating at least 10 sentences.\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f54c65",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Script to convert csv to text file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f02c0c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#This script requires to convert the TSV file to CSV\n",
    "# easiest way is to open it in Calc or excel and save as csv\n",
    "#PATH = '/Users/raulastrga/Library/Mobile Documents/com~apple~CloudDocs/Maestría/Advanced Machine Learning Methods/MNA/AMLM/Activity_4/eng-spa.tsv'\n",
    "#import pandas as pd\n",
    "#df = pd.read_csv(PATH, sep='\\t', on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "787d9408",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "#eng_spa_cols['length'] = eng_spa_cols.iloc[:, 0].str.len()  \n",
    "#eng_spa_cols = eng_spa_cols.sort_values(by='length')  \n",
    "#eng_spa_cols = eng_spa_cols.drop(columns=['length'])  \n",
    "\n",
    "#output_file_path = '/media/pepe/DataUbuntu/Databases/spanish_english/eng-spa4.txt'\n",
    "#eng_spa_cols.to_csv(output_file_path, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {},
   "source": [
    "## Import libraries required for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5dcf681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1071d90b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898a075",
   "metadata": {},
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2cbd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # GPU will be used if available\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps') # GPU will be used in Apple Silicon Macs if available\n",
    "else:\n",
    "    device = torch.device('cpu') # CPU will be used if GPU is not available\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f324a3",
   "metadata": {},
   "source": [
    "## Building the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184e618",
   "metadata": {},
   "source": [
    "[![](transformer.png)](transformer.png \"Transformer Architecture\")\n",
    "\n",
    "The Transformer model, introduced in the “Attention is All You Need” paper, revolutionizes sequence processing by relying entirely on self-attention mechanisms rather than recurrence or convolution. It consists of an encoder-decoder architecture, where both the encoder and decoder are made up of multiple layers of self-attention and feed-forward networks. The key innovation is the self-attention mechanism, which allows the model to weigh the importance of different tokens in the input sequence, regardless of their position. This enables parallelization and significantly improves efficiency, especially for tasks like machine translation. Additionally, positional encodings are used to inject sequence order information, compensating for the model’s lack of inherent positional processing. The Transformer’s architecture has since become the foundation for many state-of-the-art models in natural language processing.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb667b",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44b43119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: pending\n",
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409a9a1",
   "metadata": {},
   "source": [
    "### Positional Encoding Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025ec9a",
   "metadata": {},
   "source": [
    "[![](positional_encoding.png)](positional_encoding.png \"Positional Encoding\")\n",
    "\n",
    "Positional Encoding is a key concept introduced in the “Attention is All You Need” paper to address the lack of sequential order processing in the Transformer model. Since Transformers don’t inherently process input data in a temporal or spatial order like RNNs or CNNs, positional encodings are added to the input embeddings to inject information about the position of each token in a sequence. Typically, these encodings are generated using sinusoidal functions, where each dimension corresponds to a different frequency. This enables the model to capture the relative or absolute positions of tokens, allowing it to process sequences effectively without relying on traditional recurrence or convolution mechanisms.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9441393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        Initializes the positional embedding matrix, which adds positional information to token embeddings \n",
    "        by using sine and cosine functions as proposed in \"Attention is All You Need\".\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the embeddings for each token.\n",
    "            max_seq_len (int, optional): The maximum number of tokens in a sentence. Defaults to MAX_SEQ_LEN.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initializing a matrix for positional embeddings with zeros, with shape [max_seq_len, d_model]\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        # Token position array with shape [max_seq_len, 1] to store the positions of tokens in sequence\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "\n",
    "        # Computing the scaling term for each position, as described in the Transformer model,\n",
    "        # which controls the frequency of the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() \n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "        \n",
    "        # Assigning sine values to even indices and cosine values to odd indices in the positional embedding matrix\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "\n",
    "        # Adding a batch dimension and transposing to match expected input shape [1, max_seq_len, d_model]\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional embeddings to the input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input embeddings of shape [seq_len, batch_size, d_model].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The input embeddings with positional information added.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Adds the positional embedding matrix to the input embeddings, broadcasting over batch and sequence dimensions\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e56df",
   "metadata": {},
   "source": [
    "### Multi-Head Attention Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121da37",
   "metadata": {},
   "source": [
    "[![](multiheadattention_.png)](multiheadattention_.png \"Multi-Head Attention\")\n",
    "\n",
    "The Multi-Head Attention module in the Transformer model enhances the self-attention mechanism by allowing the model to focus on different parts of the input sequence simultaneously. Instead of performing a single attention operation, it runs multiple attention operations (or “heads”) in parallel, each with different learned attention weights. The outputs of these attention heads are then concatenated and linearly transformed to produce the final result. This approach enables the model to capture a broader range of relationships and dependencies within the data, as each attention head can focus on different aspects of the sequence. Multi-Head Attention thus improves the model’s ability to understand complex patterns and interactions in the input sequence.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54595b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        \"\"\"\n",
    "        Multi-head attention mechanism that divides attention computation into multiple heads \n",
    "        for parallelized self-attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the embeddings for each token.\n",
    "            num_heads (int): Number of attention heads for multi-head attention.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If d_model is not divisible by num_heads, as each head must \n",
    "                            have an equal share of d_model for compatibility.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "        \n",
    "        # Dimension per head for keys (d_k) and values (d_v)\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Linear transformations for query, key, and value projection\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Computes multi-head attention for given query, key, and value tensors.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): Query tensor of shape [batch_size, seq_len, num_heads*d_k].\n",
    "            K (torch.Tensor): Key tensor of shape [batch_size, seq_len, num_heads*d_k].\n",
    "            V (torch.Tensor): Value tensor of shape [batch_size, seq_len, num_heads*d_k].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains the following:\n",
    "                - torch.Tensor: Weighted values after multi-head attention and transformation.\n",
    "                - torch.Tensor: Attention scores.\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # Linear projections of Q, K, V for each head, and reshaping to [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        \n",
    "        # Scaled dot-product attention computation\n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "\n",
    "        # Concatenation of attention heads and output projection\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "    \n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention for the query, key, and value tensors.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): Query tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
    "            K (torch.Tensor): Key tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
    "            V (torch.Tensor): Value tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains the following:\n",
    "                - torch.Tensor: Weighted values after applying attention.\n",
    "                - torch.Tensor: Softmaxed attention scores.\n",
    "        \"\"\"\n",
    "        # Calculate the dot product of Q and K, scaled by sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask to the scores if provided, setting masked positions to a very low value\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Compute attention scores with softmax\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "        \n",
    "        # Calculate weighted values by multiplying attention scores with V\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "        \n",
    "        return weighted_values, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3d118",
   "metadata": {},
   "source": [
    "### Feed Forward Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095524db",
   "metadata": {},
   "source": [
    "[![](feed_forward.png)](feed_forward.png \"Feed Forward\")\n",
    "\n",
    "The Feed Forward module in the Transformer model is a fully connected layer that follows the Multi-Head Attention in both the encoder and decoder. It consists of two linear transformations with a ReLU activation in between. First, the input is passed through a linear layer, followed by a ReLU activation, and then through another linear layer. This module applies the same transformation independently to each position in the sequence, providing non-linearity and helping the model learn complex relationships. While it operates independently on each token, it allows the Transformer to process information more efficiently by adding depth and expressiveness to the model’s representation of the data.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c232006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Feed-forward network used after the self-attention mechanism in each encoder layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the input and output (embedding size).\n",
    "            d_ff (int): Dimensionality of the inner layer (hidden layer size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First linear transformation from d_model to d_ff (hidden layer size)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "         # Second linear transformation back from d_ff to d_model\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes the input through the two linear transformations with a ReLU activation \n",
    "        in between to introduce non-linearity.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, d_model].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Apply the first linear transformation followed by ReLU activation\n",
    "        x = F.relu(self.linear1(x))\n",
    "        # Apply the second linear transformation to get the final output\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ef518",
   "metadata": {},
   "source": [
    "### Encoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f60d70",
   "metadata": {},
   "source": [
    "[![](encoder.png)](encoder.png \"Encoder\")\n",
    "\n",
    "The Encoder module of the Transformer model is responsible for processing the input sequence and generating a context-aware representation of each token. It consists of a stack of identical layers, each comprising two main components: a Multi-Head Attention mechanism and a Feed Forward neural network. In each layer, the Multi-Head Attention computes attention scores to capture dependencies between tokens, while the Feed Forward network applies further transformations to each token’s representation. Both components are followed by layer normalization and residual connections to stabilize training. The encoder produces a set of encoded representations that capture the input sequence’s context, which is then passed to the decoder for further processing in tasks like machine translation.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45354c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes an EncoderSubLayer, a single layer of the encoder, which consists of self-attention and \n",
    "        feed-forward sublayers with layer normalization and dropout.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the embeddings for each token.\n",
    "            num_heads (int): Number of attention heads for multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward network in this sublayer.\n",
    "            dropout (float, optional): Dropout rate for regularization in attention and feed-forward layers. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Position-wise feed-forward network\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        # Layer normalization applied after each sublayer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Dropout layers for regularization\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask = None):\n",
    "        \"\"\"\n",
    "        Passes the input through the self-attention, feed-forward layers, with residual connections, normalization, and dropout.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, d_model].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor with the same shape as the input [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Apply self-attention with residual connection, dropout, and layer normalization\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        # Apply position-wise feed-forward network with residual connection, dropout, and layer normalization\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8a2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder, a stack of multiple encoder layers, each with self-attention and feed-forward sublayers.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the embeddings for each token.\n",
    "            num_heads (int): Number of attention heads for multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward network in each encoder layer.\n",
    "            num_layers (int): Number of encoder layers in the stack (N in the original Transformer architecture).\n",
    "            dropout (float, optional): Dropout rate applied to embeddings and feed-forward layers. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Creating a list of encoder layers, each with multi-head attention and feed-forward sublayers\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        # Final layer normalization applied after all encoder layers\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Passes input embeddings through the encoder stack, applying each encoder layer in sequence.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings of shape [batch_size, seq_len, d_model].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions, \n",
    "                                           such as padding tokens. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded representation of the input, with shape [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Sequentially applies each encoder layer, passing the output to the next layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "            \n",
    "        # Applies layer normalization to the final encoder output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4069b6",
   "metadata": {},
   "source": [
    "### Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d0dab",
   "metadata": {},
   "source": [
    "[![](decoder.png)](decoder.png \"Decoder\")\n",
    "\n",
    "The Decoder module of the Transformer model is responsible for generating the output sequence, typically used in tasks like machine translation. Like the encoder, it consists of a stack of identical layers, but with an additional layer of Multi-Head Attention. Each layer in the decoder contains three main components: a Multi-Head Attention mechanism that attends to the encoder’s output, another Multi-Head Attention that attends to the decoder’s previous layer (enabling autoregressive generation), and a Feed Forward neural network. The decoder also incorporates layer normalization and residual connections. The final output of the decoder is passed through a linear layer and softmax function to produce a probability distribution over the target vocabulary, from which the next token is predicted. This process is repeated until the entire sequence is generated.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d40ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the sublayer with self-attention, cross-attention, \n",
    "        position-wise feed-forward layers, normalization, and dropout.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of embeddings and hidden layers.\n",
    "            num_heads (int): Number of attention heads in multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward layer.\n",
    "            dropout (float): Dropout rate applied to embeddings and feed-forward layers. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Self-attention layer for the target sequence\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Cross-attention layer to attend to encoder outputs\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Position-wise feed-forward layer\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        # Layer normalizations for each subcomponent\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        # Dropout layers to prevent overfitting\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder sublayer, applying self-attention, \n",
    "        cross-attention, and a feed-forward network, with residual connections \n",
    "        and normalization.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Target sequence tensor with shape \n",
    "                (batch_size, target_seq_len, d_model).\n",
    "            encoder_output (torch.Tensor): Encoder output with shape \n",
    "                (batch_size, source_seq_len, d_model).\n",
    "            target_mask (torch.Tensor, optional): Mask for self-attention \n",
    "                within the target sequence.\n",
    "            encoder_mask (torch.Tensor, optional): Mask for cross-attention \n",
    "                with encoder output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor of shape (batch_size, target_seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # Self-attention over the target sequence with residual connection\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score) # Apply dropout and add residual\n",
    "        x = self.norm1(x) # Normalize the result\n",
    "        \n",
    "        # Cross-attention with encoder output, allowing decoder to attend to encoded source\n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)  # Apply dropout and add residual\n",
    "        x = self.norm2(x) # Normalize the result\n",
    "        \n",
    "        # Position-wise feed-forward layer with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output) # Apply dropout and add residual\n",
    "        return self.norm3(x) # Final layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer decoder, which decodes the encoded information into target sequence \n",
    "        representations while attending to the encoder output.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of embeddings and hidden layers.\n",
    "            num_heads (int): Number of attention heads in each multi-head attention layer.\n",
    "            d_ff (int): Dimension of the feed-forward layer.\n",
    "            num_layers (int): Number of decoder sub-layers.\n",
    "            dropout (float): Dropout rate applied to the decoder layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create a list of DecoderSubLayer instances, each representing one layer in the decoder stack\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Final layer normalization applied to the decoder output\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        \"\"\"\n",
    "        Passes the input through each decoder layer, allowing each layer to attend to\n",
    "        both the current input and the encoder output, then applies layer normalization.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape [batch_size, target_seq_len, d_model].\n",
    "            encoder_output (torch.Tensor): Output from the encoder with shape [batch_size, source_seq_len, d_model].\n",
    "            target_mask (torch.Tensor): Mask to prevent attending to future tokens in the target sequence.\n",
    "            encoder_mask (torch.Tensor): Mask to prevent attending to padding tokens in the encoder output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor with shape [batch_size, target_seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Pass input through each DecoderSubLayer in the decoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        # Apply final layer normalization to the output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82de2d",
   "metadata": {},
   "source": [
    "### Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61070162",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size, \n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model with embeddings, encoder, decoder, \n",
    "        and an output projection layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of embeddings and hidden layers.\n",
    "            num_heads (int): Number of attention heads in multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward network.\n",
    "            num_layers (int): Number of encoder and decoder layers.\n",
    "            input_vocab_size (int): Size of the input vocabulary.\n",
    "            target_vocab_size (int): Size of the target vocabulary.\n",
    "            max_len (int): Maximum length of input sequence.\n",
    "            dropout (float): Dropout for regularization. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer for the input vocabulary, maps each token to a vector of size d_model\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Embedding layer for the target vocabulary, maps target tokens to vectors of size d_model\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding layer to add positional information to the token embeddings, helps model learn sequence order and structure\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "\n",
    "        # Encoder module: processes the source sequence to create context-aware embeddings\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "\n",
    "        # Decoder module: generates target sequence predictions using encoder outputs as context\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "\n",
    "        # Output layer that projects decoder outputs into the target vocabulary's size, yielding logits over possible output tokens\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source, target):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): Input tensor representing the source sequence, \n",
    "                                   shape (batch_size, source_seq_len).\n",
    "            target (torch.Tensor): Input tensor representing the target sequence (used as context),\n",
    "                                   shape (batch_size, target_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with logits over the target vocabulary for each token in the target sequence,\n",
    "                          shape (batch_size, target_seq_len, target_vocab_size).\n",
    "        \"\"\"\n",
    "        # Generate masks to control which tokens the model should attend to, including padding\n",
    "        # and causal masking (for future token masking) in the target sequence\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "\n",
    "        # Embedding and positional encoding for the source sequence:\n",
    "        # scales embeddings by sqrt(d_model) to maintain variance, adds positional encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        source = self.pos_embedding(source)\n",
    "        \n",
    "        # Pass the processed source sequence through the encoder to obtain encoded representations\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        \n",
    "        # Embedding and positional encoding for the target sequence, processed similarly to source\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "        \n",
    "        # Pass the processed target sequence and encoder outputs to the decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        \n",
    "        # Project the decoder outputs to the target vocabulary, returning logits over each possible token\n",
    "        return self.output_layer(output)\n",
    "    \n",
    "    def mask(self, source, target):\n",
    "        \"\"\"\n",
    "        Creates masks to (1) ignore padding tokens in source and target sequences and \n",
    "        (2) prevent attention to future tokens in the target sequence (causal masking).\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): Tensor for the source sequence.\n",
    "            target (torch.Tensor): Tensor for the target sequence.\n",
    "\n",
    "        Returns:\n",
    "            tuple: source_mask and target_mask.\n",
    "                   - source_mask (torch.Tensor): Mask for padding tokens in the source sequence, \n",
    "                     shape (batch_size, 1, 1, source_seq_len).\n",
    "                   - target_mask (torch.Tensor): Mask for both padding tokens and future tokens in the \n",
    "                     target sequence, shape (batch_size, 1, target_seq_len, target_seq_len).\n",
    "        \"\"\"\n",
    "        # Mask to prevent attention to padding tokens in the source sequence,\n",
    "        # true values indicate non-padding tokens\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Mask to prevent attention to padding tokens in the target sequence,\n",
    "        # causal masking is applied to prevent attending to future tokens in sequence\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Define a lower-triangular matrix (causal mask) for target sequence, blocking future tokens\n",
    "        size = target.size(1) # target sequence length\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask  # combines padding and causal masking\n",
    "        \n",
    "        return source_mask, target_mask\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40581d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define the sequence length for both source and target sentences; these represent the maximum\n",
    "# number of tokens in each sentence for source and target languages\n",
    "seq_len_source = 10  # Sequence length for source sentences\n",
    "seq_len_target = 10  # Sequence length for target sentences\n",
    "\n",
    "# Define the batch size, which is the number of sentence pairs processed in parallel\n",
    "batch_size = 2  # Number of sequences (sentences) in each batch\n",
    "\n",
    "# Define the vocabulary sizes for source and target languages. Each integer represents a unique token.\n",
    "input_vocab_size = 50   # Vocabulary size for source language (input language)\n",
    "target_vocab_size = 50  # Vocabulary size for target language (output language)\n",
    "\n",
    "# Generate random sequences of token IDs for the source sentences, with integers ranging\n",
    "# from 1 to input_vocab_size. This simulates actual sentences from the input language.\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "\n",
    "# Generate random sequences of token IDs for the target sentences, with integers ranging\n",
    "# from 1 to target_vocab_size. This simulates actual sentences in the target language.\n",
    "# Shape: (batch_size, seq_len_target)\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7cf689",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6\n",
    "\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                  input_vocab_size, target_vocab_size, \n",
    "                  max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4618560e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output = model(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0bc69d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50]\n",
    "print(f'ouput.shape {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2910",
   "metadata": {},
   "source": [
    "### Translator Eng-Spa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of file of sentences english and spanish\n",
    "PATH = '/Users/raulastrga/Library/Mobile Documents/com~apple~CloudDocs/Maestría/Advanced Machine Learning Methods/MNA/AMLM/Activity_4/eng-spa.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and load file\n",
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing first 10 lines for example\n",
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separing english and spanish sentences\n",
    "eng_sentences = [pair[1] for pair in eng_spa_pairs]\n",
    "spa_sentences = [pair[3] for pair in eng_spa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing first 10 lines of every language\n",
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d11478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that preprocess a sentecen for clean special characters with regular expression\n",
    "def preprocess_sentence(sentence):\n",
    "    # Putting sentence to lowercase\n",
    "    sentence = sentence.lower().strip()\n",
    "    # Looking for one or mor spaces and changing for only one\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # Looking for special characters or numbers and changing for regular character or space\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    # Deleting spaces at start or end of the sentence\n",
    "    sentence = sentence.strip()\n",
    "    # Adding tags for start and end the sentence\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a example of sentence with special characters\n",
    "s1 = '¿Hola @ cómo estás? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the example\n",
    "print(s1)\n",
    "\n",
    "# Preproccess the previous example and printing result\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the full list of sentencens of every language\n",
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing first 10 examples of spanish list\n",
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97931cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for create vocabulary \n",
    "def build_vocab(sentences):\n",
    "    # Separing every word in every sentence and create a list of words\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    # Counting all words and how much is repeting \n",
    "    word_count = Counter(words)\n",
    "    # Order words, more counts first \n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
    "    # Creating dictionary of words and indexes\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    # Adding Padding at first position\n",
    "    word2idx['<pad>'] = 0\n",
    "    # Adding Unknown at second position\n",
    "    word2idx['<unk>'] = 1\n",
    "    # Creating dictionary on reverse indexes and words\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vocabularies for english and spanish\n",
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "\n",
    "# Creating variables with size of both vocabularies\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "spa_vocab_size = len(spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing both vocabularies size\n",
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        self.eng_sentences = eng_sentences\n",
    "        self.spa_sentences = spa_sentences\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.spa_word2idx = spa_word2idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.eng_sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "        # return tokens idxs\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "        \n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
    "    return eng_batch, spa_batch\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d514b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0 \n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "            # Decoder preprocessing\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "            # Zero grads\n",
    "            optimiser.zero_grad()\n",
    "            # run model\n",
    "            output = model(eng_batch, target_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "            # loss\\\n",
    "            loss = loss_function(output, target_output)\n",
    "            # gradient and update parameters\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss/len(dataloader)\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08eef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1181a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e265e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader, loss_function, optimiser, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d271146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50740746",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word2idx):\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
    "\n",
    "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    model.eval()\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize the target tensor with <sos> token\n",
    "    tgt_indices = [spa_word2idx['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "            output = output.squeeze(0)\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "            if next_token == spa_word2idx['<eos>']:\n",
    "                break\n",
    "\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0db72",
   "metadata": {
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    for sentence in sentences:\n",
    "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
    "        print(f'Input sentence: {sentence}')\n",
    "        print(f'Traducción: {translation}')\n",
    "        print()\n",
    "\n",
    "# Example sentences to test the translator\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning artificial intelligence.\",\n",
    "    \"Artificial intelligence is great.\",\n",
    "    \"Good night!\"\n",
    "]\n",
    "\n",
    "# Assuming the model is trained and loaded\n",
    "# Set the device to 'cpu' or 'cuda' as needed\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate translations\n",
    "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceefe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e10a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb7af9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6321db74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccce7864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
