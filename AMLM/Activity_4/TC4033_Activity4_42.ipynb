{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e38b863",
   "metadata": {
    "id": "5e38b863"
   },
   "source": [
    "# Team members"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384b5edb",
   "metadata": {
    "id": "384b5edb"
   },
   "source": [
    "| Id        | Student                                 |\n",
    "|-----------|-----------------------------------------|\n",
    "| A01795654 | Raul Astorga Castro                     |\n",
    "| A01795579 | Edson Misael Astorga Castro             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
   "metadata": {
    "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9"
   },
   "source": [
    "# TC 5033\n",
    "## Deep Learning\n",
    "## Transformers\n",
    "\n",
    "## Activity 4: Implementing a Translator\n",
    "\n",
    "- Objective\n",
    "\n",
    "To understand the Transformer Architecture by Implementing a translator.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "    This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "    Follow the provided code. The code already implements a transformer from scratch as explained in one of [week's 9 videos](https://youtu.be/XefFj4rLHgU)\n",
    "\n",
    "    Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells.  You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
    "  \n",
    "- Evaluation Criteria\n",
    "\n",
    "    - Code Readability and Comments\n",
    "    - Traning a translator\n",
    "    - Translating at least 10 sentences.\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {
    "id": "7d468e9a"
   },
   "source": [
    "## Import libraries required for the model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5dcf681",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5dcf681",
    "outputId": "b91ec2fe-ac58-4579-a6ec-3069cf3c7998"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x79ca385c6ff0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898a075",
   "metadata": {
    "id": "0898a075"
   },
   "source": [
    "## Using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c2cbd17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2c2cbd17",
    "outputId": "ce77f502-1329-4dff-b1f1-ab0466a7bbd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda') # GPU will be used if available\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps') # GPU will be used in Apple Silicon Macs if available\n",
    "else:\n",
    "    device = torch.device('cpu') # CPU will be used if GPU is not available\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f324a3",
   "metadata": {
    "id": "60f324a3"
   },
   "source": [
    "## Building the Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184e618",
   "metadata": {
    "id": "4184e618"
   },
   "source": [
    "[![](transformer.png)](transformer.png \"Transformer Architecture\")\n",
    "\n",
    "The Transformer model, introduced in the “Attention is All You Need” paper, revolutionizes sequence processing by relying entirely on self-attention mechanisms rather than recurrence or convolution. It consists of an encoder-decoder architecture, where both the encoder and decoder are made up of multiple layers of self-attention and feed-forward networks. The key innovation is the self-attention mechanism, which allows the model to weigh the importance of different tokens in the input sequence, regardless of their position. This enables parallelization and significantly improves efficiency, especially for tasks like machine translation. Additionally, positional encodings are used to inject sequence order information, compensating for the model’s lack of inherent positional processing. The Transformer’s architecture has since become the foundation for many state-of-the-art models in natural language processing.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb667b",
   "metadata": {
    "id": "8ecb667b"
   },
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44b43119",
   "metadata": {
    "id": "44b43119"
   },
   "outputs": [],
   "source": [
    "# TODO: pending\n",
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b409a9a1",
   "metadata": {
    "id": "b409a9a1"
   },
   "source": [
    "### Positional Encoding Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025ec9a",
   "metadata": {
    "id": "e025ec9a"
   },
   "source": [
    "[![](positional_encoding.png)](positional_encoding.png \"Positional Encoding\")\n",
    "\n",
    "Positional Encoding is a key concept introduced in the “Attention is All You Need” paper to address the lack of sequential order processing in the Transformer model. Since Transformers don’t inherently process input data in a temporal or spatial order like RNNs or CNNs, positional encodings are added to the input embeddings to inject information about the position of each token in a sequence. Typically, these encodings are generated using sinusoidal functions, where each dimension corresponds to a different frequency. This enables the model to capture the relative or absolute positions of tokens, allowing it to process sequences effectively without relying on traditional recurrence or convolution mechanisms.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9441393f",
   "metadata": {
    "id": "9441393f"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len = MAX_SEQ_LEN):\n",
    "        \"\"\"\n",
    "        Initializes the positional embedding matrix, which adds positional information to token embeddings\n",
    "        by using sine and cosine functions as proposed in \"Attention is All You Need\".\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The dimensionality of the embeddings for each token.\n",
    "            max_seq_len (int, optional): The maximum number of tokens in a sentence. Defaults to MAX_SEQ_LEN.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initializing a matrix for positional embeddings with zeros, with shape [max_seq_len, d_model]\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "        # Token position array with shape [max_seq_len, 1] to store the positions of tokens in sequence\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype = torch.float).unsqueeze(1)\n",
    "\n",
    "        # Computing the scaling term for each position, as described in the Transformer model,\n",
    "        # which controls the frequency of the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float()\n",
    "                             * (-math.log(10000.0)/d_model))\n",
    "\n",
    "        # Assigning sine values to even indices and cosine values to odd indices in the positional embedding matrix\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "\n",
    "        # Adding a batch dimension and transposing to match expected input shape [1, max_seq_len, d_model]\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional embeddings to the input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input embeddings of shape [seq_len, batch_size, d_model].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The input embeddings with positional information added.\n",
    "        \"\"\"\n",
    "\n",
    "        # Adds the positional embedding matrix to the input embeddings, broadcasting over batch and sequence dimensions\n",
    "        return x + self.pos_embed_matrix[:x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e56df",
   "metadata": {
    "id": "662e56df"
   },
   "source": [
    "### Multi-Head Attention Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121da37",
   "metadata": {
    "id": "c121da37"
   },
   "source": [
    "Module                                                                          |  Architecture\n",
    ":------------------------------------------------------------------------------:|:--------------------------------------------------------------------------------:\n",
    "[![](multiheadattention_.png)](multiheadattention_.png \"Multi-Head Attention\")  |  [![](multiheadattention_2.png)](multiheadattention_2.png \"Multi-Head Attention\")\n",
    "\n",
    "\n",
    "The Multi-Head Attention module in the Transformer model enhances the self-attention mechanism by allowing the model to focus on different parts of the input sequence simultaneously. Instead of performing a single attention operation, it runs multiple attention operations (or “heads”) in parallel, each with different learned attention weights. The outputs of these attention heads are then concatenated and linearly transformed to produce the final result. This approach enables the model to capture a broader range of relationships and dependencies within the data, as each attention head can focus on different aspects of the sequence. Multi-Head Attention thus improves the model’s ability to understand complex patterns and interactions in the input sequence.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54595b67",
   "metadata": {
    "id": "54595b67"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model = 512, num_heads = 8):\n",
    "        \"\"\"\n",
    "        Multi-head attention mechanism that divides attention computation into multiple heads\n",
    "        for parallelized self-attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the embeddings for each token.\n",
    "            num_heads (int): Number of attention heads for multi-head attention.\n",
    "\n",
    "        Raises:\n",
    "            AssertionError: If d_model is not divisible by num_heads, as each head must\n",
    "                            have an equal share of d_model for compatibility.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, 'Embedding size not compatible with num heads'\n",
    "\n",
    "        # Dimension per head for keys (d_k) and values (d_v)\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Linear transformations for query, key, and value projection\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Computes multi-head attention for given query, key, and value tensors.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): Query tensor of shape [batch_size, seq_len, num_heads*d_k].\n",
    "            K (torch.Tensor): Key tensor of shape [batch_size, seq_len, num_heads*d_k].\n",
    "            V (torch.Tensor): Value tensor of shape [batch_size, seq_len, num_heads*d_k].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains the following:\n",
    "                - torch.Tensor: Weighted values after multi-head attention and transformation.\n",
    "                - torch.Tensor: Attention scores.\n",
    "        \"\"\"\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        # Linear projections of Q, K, V for each head, and reshaping to [batch_size, num_heads, seq_len, d_k]\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2 )\n",
    "\n",
    "        # Scaled dot-product attention computation\n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "\n",
    "        # Concatenation of attention heads and output projection\n",
    "        weighted_values = weighted_values.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads*self.d_k)\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "\n",
    "        return weighted_values, attention\n",
    "\n",
    "    def scale_dot_product(self, Q, K, V, mask = None):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention for the query, key, and value tensors.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): Query tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
    "            K (torch.Tensor): Key tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
    "            V (torch.Tensor): Value tensor of shape [batch_size, num_heads, seq_len, d_k].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Contains the following:\n",
    "                - torch.Tensor: Weighted values after applying attention.\n",
    "                - torch.Tensor: Softmaxed attention scores.\n",
    "        \"\"\"\n",
    "        # Calculate the dot product of Q and K, scaled by sqrt(d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask to the scores if provided, setting masked positions to a very low value\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Compute attention scores with softmax\n",
    "        attention = F.softmax(scores, dim = -1)\n",
    "\n",
    "        # Calculate weighted values by multiplying attention scores with V\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "\n",
    "        return weighted_values, attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea3d118",
   "metadata": {
    "id": "6ea3d118"
   },
   "source": [
    "### Feed Forward Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095524db",
   "metadata": {
    "id": "095524db"
   },
   "source": [
    "[![](feed_forward.png)](feed_forward.png \"Feed Forward\")\n",
    "\n",
    "The Feed Forward module in the Transformer model is a fully connected layer that follows the Multi-Head Attention in both the encoder and decoder. It consists of two linear transformations with a ReLU activation in between. First, the input is passed through a linear layer, followed by a ReLU activation, and then through another linear layer. This module applies the same transformation independently to each position in the sequence, providing non-linearity and helping the model learn complex relationships. While it operates independently on each token, it allows the Transformer to process information more efficiently by adding depth and expressiveness to the model’s representation of the data.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c232006",
   "metadata": {
    "id": "6c232006"
   },
   "outputs": [],
   "source": [
    "class PositionFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Feed-forward network used after the self-attention mechanism in each encoder layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the input and output (embedding size).\n",
    "            d_ff (int): Dimensionality of the inner layer (hidden layer size).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First linear transformation from d_model to d_ff (hidden layer size)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "         # Second linear transformation back from d_ff to d_model\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes the input through the two linear transformations with a ReLU activation\n",
    "        in between to introduce non-linearity.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, d_model].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of the same shape [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Apply the first linear transformation followed by ReLU activation\n",
    "        x = F.relu(self.linear1(x))\n",
    "        # Apply the second linear transformation to get the final output\n",
    "        return self.linear2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4ef518",
   "metadata": {
    "id": "fc4ef518"
   },
   "source": [
    "### Encoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f60d70",
   "metadata": {
    "id": "a5f60d70"
   },
   "source": [
    "[![](encoder.png)](encoder.png \"Encoder\")\n",
    "\n",
    "The Encoder module of the Transformer model is responsible for processing the input sequence and generating a context-aware representation of each token. It consists of a stack of identical layers, each comprising two main components: a Multi-Head Attention mechanism and a Feed Forward neural network. In each layer, the Multi-Head Attention computes attention scores to capture dependencies between tokens, while the Feed Forward network applies further transformations to each token’s representation. Both components are followed by layer normalization and residual connections to stabilize training. The encoder produces a set of encoded representations that capture the input sequence’s context, which is then passed to the decoder for further processing in tasks like machine translation.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45354c6f",
   "metadata": {
    "id": "45354c6f"
   },
   "outputs": [],
   "source": [
    "class EncoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes an EncoderSubLayer, a single layer of the encoder, which consists of self-attention and\n",
    "        feed-forward sublayers with layer normalization and dropout.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the embeddings for each token.\n",
    "            num_heads (int): Number of attention heads for multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward network in this sublayer.\n",
    "            dropout (float, optional): Dropout rate for regularization in attention and feed-forward layers. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Multi-head self-attention mechanism\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Position-wise feed-forward network\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        # Layer normalization applied after each sublayer\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Dropout layers for regularization\n",
    "        self.droupout1 = nn.Dropout(dropout)\n",
    "        self.droupout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask = None):\n",
    "        \"\"\"\n",
    "        Passes the input through the self-attention, feed-forward layers, with residual connections, normalization, and dropout.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape [batch_size, seq_len, d_model].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor with the same shape as the input [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Apply self-attention with residual connection, dropout, and layer normalization\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.droupout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "        # Apply position-wise feed-forward network with residual connection, dropout, and layer normalization\n",
    "        x = x + self.droupout2(self.ffn(x))\n",
    "        return self.norm2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da8a2b71",
   "metadata": {
    "id": "da8a2b71"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Encoder, a stack of multiple encoder layers, each with self-attention and feed-forward sublayers.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of the embeddings for each token.\n",
    "            num_heads (int): Number of attention heads for multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward network in each encoder layer.\n",
    "            num_layers (int): Number of encoder layers in the stack (N in the original Transformer architecture).\n",
    "            dropout (float, optional): Dropout rate applied to embeddings and feed-forward layers. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Creating a list of encoder layers, each with multi-head attention and feed-forward sublayers\n",
    "        self.layers = nn.ModuleList([EncoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        # Final layer normalization applied after all encoder layers\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Passes input embeddings through the encoder stack, applying each encoder layer in sequence.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input embeddings of shape [batch_size, seq_len, d_model].\n",
    "            mask (torch.Tensor, optional): Mask tensor to prevent attention to certain positions,\n",
    "                                           such as padding tokens. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded representation of the input, with shape [batch_size, seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Sequentially applies each encoder layer, passing the output to the next layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        # Applies layer normalization to the final encoder output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4069b6",
   "metadata": {
    "id": "1e4069b6"
   },
   "source": [
    "### Decoder Module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d0dab",
   "metadata": {
    "id": "555d0dab"
   },
   "source": [
    "[![](decoder.png)](decoder.png \"Decoder\")\n",
    "\n",
    "The Decoder module of the Transformer model is responsible for generating the output sequence, typically used in tasks like machine translation. Like the encoder, it consists of a stack of identical layers, but with an additional layer of Multi-Head Attention. Each layer in the decoder contains three main components: a Multi-Head Attention mechanism that attends to the encoder’s output, another Multi-Head Attention that attends to the decoder’s previous layer (enabling autoregressive generation), and a Feed Forward neural network. The decoder also incorporates layer normalization and residual connections. The final output of the decoder is passed through a linear layer and softmax function to produce a probability distribution over the target vocabulary, from which the next token is predicted. This process is repeated until the entire sequence is generated.\n",
    "\n",
    "> Vaswani, A. et al. (2017). Attention is All You NeedLinks to an external site. https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "015d40ca",
   "metadata": {
    "id": "015d40ca"
   },
   "outputs": [],
   "source": [
    "class DecoderSubLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the sublayer with self-attention, cross-attention,\n",
    "        position-wise feed-forward layers, normalization, and dropout.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of embeddings and hidden layers.\n",
    "            num_heads (int): Number of attention heads in multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward layer.\n",
    "            dropout (float): Dropout rate applied to embeddings and feed-forward layers. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Self-attention layer for the target sequence\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Cross-attention layer to attend to encoder outputs\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        # Position-wise feed-forward layer\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        # Layer normalizations for each subcomponent\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        # Dropout layers to prevent overfitting\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask=None, encoder_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the decoder sublayer, applying self-attention,\n",
    "        cross-attention, and a feed-forward network, with residual connections\n",
    "        and normalization.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Target sequence tensor with shape\n",
    "                (batch_size, target_seq_len, d_model).\n",
    "            encoder_output (torch.Tensor): Encoder output with shape\n",
    "                (batch_size, source_seq_len, d_model).\n",
    "            target_mask (torch.Tensor, optional): Mask for self-attention\n",
    "                within the target sequence.\n",
    "            encoder_mask (torch.Tensor, optional): Mask for cross-attention\n",
    "                with encoder output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor of shape (batch_size, target_seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # Self-attention over the target sequence with residual connection\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score) # Apply dropout and add residual\n",
    "        x = self.norm1(x) # Normalize the result\n",
    "\n",
    "        # Cross-attention with encoder output, allowing decoder to attend to encoded source\n",
    "        encoder_attn, _ = self.cross_attn(x, encoder_output, encoder_output, encoder_mask)\n",
    "        x = x + self.dropout2(encoder_attn)  # Apply dropout and add residual\n",
    "        x = self.norm2(x) # Normalize the result\n",
    "\n",
    "        # Position-wise feed-forward layer with residual connection\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output) # Apply dropout and add residual\n",
    "        return self.norm3(x) # Final layer normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ],
    "id": "3103d45f"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer decoder, which decodes the encoded information into target sequence\n",
    "        representations while attending to the encoder output.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of embeddings and hidden layers.\n",
    "            num_heads (int): Number of attention heads in each multi-head attention layer.\n",
    "            d_ff (int): Dimension of the feed-forward layer.\n",
    "            num_layers (int): Number of decoder sub-layers.\n",
    "            dropout (float): Dropout rate applied to the decoder layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Create a list of DecoderSubLayer instances, each representing one layer in the decoder stack\n",
    "        self.layers = nn.ModuleList([DecoderSubLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        # Final layer normalization applied to the decoder output\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, encoder_output, target_mask, encoder_mask):\n",
    "        \"\"\"\n",
    "        Passes the input through each decoder layer, allowing each layer to attend to\n",
    "        both the current input and the encoder output, then applies layer normalization.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor with shape [batch_size, target_seq_len, d_model].\n",
    "            encoder_output (torch.Tensor): Output from the encoder with shape [batch_size, source_seq_len, d_model].\n",
    "            target_mask (torch.Tensor): Mask to prevent attending to future tokens in the target sequence.\n",
    "            encoder_mask (torch.Tensor): Mask to prevent attending to padding tokens in the encoder output.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Processed tensor with shape [batch_size, target_seq_len, d_model].\n",
    "        \"\"\"\n",
    "        # Pass input through each DecoderSubLayer in the decoder\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "        # Apply final layer normalization to the output\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82de2d",
   "metadata": {
    "id": "3e82de2d"
   },
   "source": [
    "### Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61070162",
   "metadata": {
    "code_folding": [],
    "id": "61070162"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers,\n",
    "                 input_vocab_size, target_vocab_size,\n",
    "                 max_len=MAX_SEQ_LEN, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model with embeddings, encoder, decoder,\n",
    "        and an output projection layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimensionality of embeddings and hidden layers.\n",
    "            num_heads (int): Number of attention heads in multi-head attention.\n",
    "            d_ff (int): Dimensionality of the feed-forward network.\n",
    "            num_layers (int): Number of encoder and decoder layers.\n",
    "            input_vocab_size (int): Size of the input vocabulary.\n",
    "            target_vocab_size (int): Size of the target vocabulary.\n",
    "            max_len (int): Maximum length of input sequence.\n",
    "            dropout (float): Dropout for regularization. Defaults to 0.1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layer for the input vocabulary, maps each token to a vector of size d_model\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # Embedding layer for the target vocabulary, maps target tokens to vectors of size d_model\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding layer to add positional information to the token embeddings, helps model learn sequence order and structure\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "\n",
    "        # Encoder module: processes the source sequence to create context-aware embeddings\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "\n",
    "        # Decoder module: generates target sequence predictions using encoder outputs as context\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "\n",
    "        # Output layer that projects decoder outputs into the target vocabulary's size, yielding logits over possible output tokens\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): Input tensor representing the source sequence,\n",
    "                                   shape (batch_size, source_seq_len).\n",
    "            target (torch.Tensor): Input tensor representing the target sequence (used as context),\n",
    "                                   shape (batch_size, target_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor with logits over the target vocabulary for each token in the target sequence,\n",
    "                          shape (batch_size, target_seq_len, target_vocab_size).\n",
    "        \"\"\"\n",
    "        # Generate masks to control which tokens the model should attend to, including padding\n",
    "        # and causal masking (for future token masking) in the target sequence\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "\n",
    "        # Embedding and positional encoding for the source sequence:\n",
    "        # scales embeddings by sqrt(d_model) to maintain variance, adds positional encoding\n",
    "        source = self.encoder_embedding(source) * math.sqrt(self.encoder_embedding.embedding_dim)\n",
    "        source = self.pos_embedding(source)\n",
    "\n",
    "        # Pass the processed source sequence through the encoder to obtain encoded representations\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "\n",
    "        # Embedding and positional encoding for the target sequence, processed similarly to source\n",
    "        target = self.decoder_embedding(target) * math.sqrt(self.decoder_embedding.embedding_dim)\n",
    "        target = self.pos_embedding(target)\n",
    "\n",
    "        # Pass the processed target sequence and encoder outputs to the decoder\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "\n",
    "        # Project the decoder outputs to the target vocabulary, returning logits over each possible token\n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def mask(self, source, target):\n",
    "        \"\"\"\n",
    "        Creates masks to (1) ignore padding tokens in source and target sequences and\n",
    "        (2) prevent attention to future tokens in the target sequence (causal masking).\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): Tensor for the source sequence.\n",
    "            target (torch.Tensor): Tensor for the target sequence.\n",
    "\n",
    "        Returns:\n",
    "            tuple: source_mask and target_mask.\n",
    "                   - source_mask (torch.Tensor): Mask for padding tokens in the source sequence,\n",
    "                     shape (batch_size, 1, 1, source_seq_len).\n",
    "                   - target_mask (torch.Tensor): Mask for both padding tokens and future tokens in the\n",
    "                     target sequence, shape (batch_size, 1, target_seq_len, target_seq_len).\n",
    "        \"\"\"\n",
    "        # Mask to prevent attention to padding tokens in the source sequence,\n",
    "        # true values indicate non-padding tokens\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Mask to prevent attention to padding tokens in the target sequence,\n",
    "        # causal masking is applied to prevent attending to future tokens in sequence\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        # Define a lower-triangular matrix (causal mask) for target sequence, blocking future tokens\n",
    "        size = target.size(1) # target sequence length\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask  # combines padding and causal masking\n",
    "\n",
    "        return source_mask, target_mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2d4",
   "metadata": {
    "heading_collapsed": true,
    "id": "6da6b2d4"
   },
   "source": [
    "## Running a Simple test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0652ae7",
   "metadata": {
    "id": "d0652ae7"
   },
   "source": [
    "### Explanation\n",
    "This simple test have the tarjet of creating synthetic data that test model correctness functionality. The main idea is provide random data simulating real words and providing them to the model.\n",
    "\n",
    "The expected output is a tensor which deliver the word index that would match the best on spanish.\n",
    "\n",
    "All the required variables are defined to run the correct simalation of this test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b9ad8",
   "metadata": {
    "id": "6a4b9ad8"
   },
   "source": [
    "### Simple Test Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d40581d6",
   "metadata": {
    "hidden": true,
    "id": "d40581d6"
   },
   "outputs": [],
   "source": [
    "# Define the sequence length for both source and target sentences; these represent the maximum\n",
    "# number of tokens in each sentence for source and target languages\n",
    "seq_len_source = 10  # Sequence length for source sentences\n",
    "seq_len_target = 10  # Sequence length for target sentences\n",
    "\n",
    "# Define the batch size, which is the number of sentence pairs processed in parallel\n",
    "batch_size = 2  # Number of sequences (sentences) in each batch\n",
    "\n",
    "# Define the vocabulary sizes for source and target languages. Each integer represents a unique token.\n",
    "input_vocab_size = 50   # Vocabulary size for source language (input language)\n",
    "target_vocab_size = 50  # Vocabulary size for target language (output language)\n",
    "\n",
    "# Generate random sequences of token IDs for the source sentences, with integers ranging\n",
    "# from 1 to input_vocab_size. This simulates actual sentences from the input language.\n",
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "\n",
    "# Generate random sequences of token IDs for the target sentences, with integers ranging\n",
    "# from 1 to target_vocab_size. This simulates actual sentences in the target language.\n",
    "# Shape: (batch_size, seq_len_target)\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638ddf7",
   "metadata": {
    "id": "b638ddf7"
   },
   "source": [
    "### Running simple test with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc7cf689",
   "metadata": {
    "hidden": true,
    "id": "fc7cf689"
   },
   "outputs": [],
   "source": [
    "# Transformer model hyperparameters: embedding dimension, attention heads, feed-forward layer size, and layers\n",
    "d_model = 512        # Dimension of embeddings\n",
    "num_heads = 8        # Number of attention heads\n",
    "d_ff = 2048          # Size of feed-forward network\n",
    "num_layers = 6       # Number of layers in encoder and decoder\n",
    "\n",
    "# Initialize the Transformer model with the specified configuration, including input and target vocab sizes\n",
    "model = Transformer(d_model, num_heads, d_ff, num_layers,\n",
    "                    input_vocab_size, target_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)\n",
    "\n",
    "# Move model and input data to the specified device (e.g., GPU) for processing\n",
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be4613",
   "metadata": {
    "id": "99be4613"
   },
   "source": [
    "### Output of Simple Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4618560e",
   "metadata": {
    "hidden": true,
    "id": "4618560e"
   },
   "outputs": [],
   "source": [
    "# Run the model forward pass to get predictions\n",
    "output = model(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab0bc69d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "ab0bc69d",
    "outputId": "477ff711-ee71-4e59-aa06-58f6ef4e655b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouput.shape torch.Size([2, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "# Expected output shape -> [batch, seq_len_target, target_vocab_size] i.e. [2, 10, 50], where each token in the target sequence\n",
    "# is represented by its probability distribution across the target vocabulary.\n",
    "print(f'ouput.shape {output.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2910",
   "metadata": {
    "id": "0f4b2910"
   },
   "source": [
    "## Translator Eng-Spa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c42b9b",
   "metadata": {
    "id": "15c42b9b"
   },
   "source": [
    "### Explanation\n",
    "\n",
    "This translator eng-spa is created on training the transformer model written above. The main idea is provide the eng-spa.tsv doc that contains couples of words: first on english and second on the posible translation in spanish.\n",
    "\n",
    "In order to be able to translate the words, both english and spanish, are corrected removing special signs and setting them on lowercase.\n",
    "\n",
    "Once the words are cleaned the function build_vocab iterates all the sentences and find all the words on the dictionary. Addionally, index to word dictionaries are created to exchange between tokens and words for english or spanish.\n",
    "\n",
    "EngSpaDataset is the dataset that returns the tokens index of the words when a sentence is provided.\n",
    "\n",
    "The train function iterates the model and calculates the loss function to improve the weights of the transformer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74daff0",
   "metadata": {
    "id": "b74daff0"
   },
   "source": [
    "### Loading parameters and vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "869a7244",
   "metadata": {
    "id": "869a7244"
   },
   "outputs": [],
   "source": [
    "# Path to the file containing English-Spanish sentence pairs\n",
    "# File downloaded from: https://tatoeba.org/en/downloads\n",
    "PATH = 'eng-spa.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0af1eba",
   "metadata": {
    "id": "d0af1eba"
   },
   "outputs": [],
   "source": [
    "# Open the file, read each line, and split English and Spanish sentence pairs by tabs\n",
    "with open(PATH, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "eng_spa_pairs = [line.strip().split('\\t') for line in lines if '\\t' in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c930226f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c930226f",
    "outputId": "7e0e4a20-87cc-46d8-8964-f000d72396fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['\\ufeff1276', \"Let's try something.\", '2481', '¡Intentemos algo!'],\n",
       " ['1277', 'I have to go to sleep.', '2482', 'Tengo que irme a dormir.'],\n",
       " ['1280',\n",
       "  \"Today is June 18th and it is Muiriel's birthday!\",\n",
       "  '2485',\n",
       "  '¡Hoy es 18 de junio y es el cumpleaños de Muiriel!'],\n",
       " ['1280',\n",
       "  \"Today is June 18th and it is Muiriel's birthday!\",\n",
       "  '1130137',\n",
       "  '¡Hoy es el 18 de junio y es el cumpleaños de Muiriel!'],\n",
       " ['1282', 'Muiriel is 20 now.', '2487', 'Ahora, Muiriel tiene 20 años.'],\n",
       " ['1282', 'Muiriel is 20 now.', '1130133', 'Muiriel tiene 20 años ahora.'],\n",
       " ['1283', 'The password is \"Muiriel\".', '2488', 'La contraseña es \"Muiriel\".'],\n",
       " ['1284', 'I will be back soon.', '2489', 'Volveré pronto.'],\n",
       " ['1284', 'I will be back soon.', '586853', 'Vuelvo en seguida.'],\n",
       " ['1284', 'I will be back soon.', '1031885', 'Yo regresaré pronto.']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first 10 sentence pairs to examine the data format\n",
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "095f4037",
   "metadata": {
    "id": "095f4037"
   },
   "outputs": [],
   "source": [
    "# Separate English and Spanish sentences into distinct lists\n",
    "eng_sentences = [pair[1] for pair in eng_spa_pairs]  # English sentences\n",
    "spa_sentences = [pair[3] for pair in eng_spa_pairs]  # Spanish sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d9e1c95",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0d9e1c95",
    "outputId": "a0c9e757-e7fe-4741-f597-0785b6756086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's try something.\", 'I have to go to sleep.', \"Today is June 18th and it is Muiriel's birthday!\", \"Today is June 18th and it is Muiriel's birthday!\", 'Muiriel is 20 now.', 'Muiriel is 20 now.', 'The password is \"Muiriel\".', 'I will be back soon.', 'I will be back soon.', 'I will be back soon.']\n",
      "['¡Intentemos algo!', 'Tengo que irme a dormir.', '¡Hoy es 18 de junio y es el cumpleaños de Muiriel!', '¡Hoy es el 18 de junio y es el cumpleaños de Muiriel!', 'Ahora, Muiriel tiene 20 años.', 'Muiriel tiene 20 años ahora.', 'La contraseña es \"Muiriel\".', 'Volveré pronto.', 'Vuelvo en seguida.', 'Yo regresaré pronto.']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 sentences from each language list for verification\n",
    "print(eng_sentences[:10])  # First 10 English sentences\n",
    "print(spa_sentences[:10])  # First 10 Spanish sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d2499",
   "metadata": {
    "id": "336d2499"
   },
   "source": [
    "### Preprocessing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60d11478",
   "metadata": {
    "id": "60d11478"
   },
   "outputs": [],
   "source": [
    "# Define a function to preprocess a sentence by removing special characters and formatting\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"\n",
    "    Preprocess a sentence by formatting, normalizing, and adding sequence tags.\n",
    "\n",
    "    This function cleans and formats an input sentence by:\n",
    "    - Converting to lowercase.\n",
    "    - Removing leading and trailing whitespace.\n",
    "    - Replacing multiple spaces with a single space.\n",
    "    - Normalizing accented characters.\n",
    "    - Removing numbers and special characters, retaining only alphabetic characters.\n",
    "    - Adding start-of-sequence (<sos>) and end-of-sequence (<eos>) tags.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The input sentence to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    str\n",
    "        The preprocessed sentence, formatted and with <sos> and <eos> tags.\n",
    "\n",
    "    Example:\n",
    "    -------\n",
    "    >>> preprocess_sentence(\"¿Hola @ cómo estás? 123\")\n",
    "    '<sos> hola como estas <eos>'\n",
    "    \"\"\"\n",
    "    # Convert sentence to lowercase and strip leading/trailing whitespace\n",
    "    sentence = sentence.lower().strip()\n",
    "    # Replace multiple spaces with a single space\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # Normalize accented characters and remove numbers or special symbols\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    # Strip extra spaces and add start-of-sequence and end-of-sequence tags\n",
    "    sentence = sentence.strip()\n",
    "    sentence = '<sos> ' + sentence + ' <eos>'\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "478f673b",
   "metadata": {
    "id": "478f673b"
   },
   "outputs": [],
   "source": [
    "# Create a sample sentence with special characters for demonstration\n",
    "s1 = '¿Hola @ cómo estás? 123'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "96ac79c5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "96ac79c5",
    "outputId": "c3c8e0e0-9c3b-4bf0-9e3f-26c1e1c8c0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Hola @ cómo estás? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "# Print the original sample sentence\n",
    "print(s1)\n",
    "\n",
    "# Preprocess the sample sentence and print the cleaned result\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9fc9c4d",
   "metadata": {
    "id": "d9fc9c4d"
   },
   "outputs": [],
   "source": [
    "# Apply the preprocess function to all English and Spanish sentences in their respective lists\n",
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7a3b18d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7a3b18d",
    "outputId": "388ed96b-5240-4ea0-8bd3-400d8d2821a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos> intentemos algo <eos>', '<sos> tengo que irme a dormir <eos>', '<sos> hoy es de junio y es el cumplea os de muiriel <eos>', '<sos> hoy es el de junio y es el cumplea os de muiriel <eos>', '<sos> ahora muiriel tiene a os <eos>', '<sos> muiriel tiene a os ahora <eos>', '<sos> la contrase a es muiriel <eos>', '<sos> volvere pronto <eos>', '<sos> vuelvo en seguida <eos>', '<sos> yo regresare pronto <eos>']\n",
      "['<sos> let s try something <eos>', '<sos> i have to go to sleep <eos>', '<sos> today is june th and it is muiriel s birthday <eos>', '<sos> today is june th and it is muiriel s birthday <eos>', '<sos> muiriel is now <eos>', '<sos> muiriel is now <eos>', '<sos> the password is muiriel <eos>', '<sos> i will be back soon <eos>', '<sos> i will be back soon <eos>', '<sos> i will be back soon <eos>']\n"
     ]
    }
   ],
   "source": [
    "# Display the first 10 preprocessed sentences of each language as examples\n",
    "print(spa_sentences[:10])\n",
    "print(eng_sentences[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8525595",
   "metadata": {
    "id": "f8525595"
   },
   "source": [
    "### Building vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97931cd3",
   "metadata": {
    "id": "97931cd3"
   },
   "outputs": [],
   "source": [
    "# Function to create a vocabulary from a list of sentences\n",
    "def build_vocab(sentences):\n",
    "    \"\"\"\n",
    "    Build a vocabulary from a list of sentences.\n",
    "\n",
    "    This function creates a vocabulary dictionary that maps each unique word in the\n",
    "    provided list of sentences to a unique index, sorted by word frequency in\n",
    "    descending order. Two special tokens are added:\n",
    "    - '<pad>' at index 0 for padding purposes.\n",
    "    - '<unk>' at index 1 for unknown or out-of-vocabulary words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : list of str\n",
    "        List of sentences used to build the vocabulary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (dict, dict)\n",
    "        - word2idx : dict\n",
    "            A dictionary mapping each word to a unique index.\n",
    "        - idx2word : dict\n",
    "            A reverse dictionary mapping indexes back to words.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> sentences = [\"hello world\", \"hello machine learning\"]\n",
    "    >>> word2idx, idx2word = build_vocab(sentences)\n",
    "    >>> word2idx[\"hello\"]\n",
    "    2\n",
    "    >>> idx2word[2]\n",
    "    'hello'\n",
    "    \"\"\"\n",
    "    # Spl\n",
    "    # Split each sentence into words and collect all words in a list\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    # Count the occurrences of each word in the list\n",
    "    word_count = Counter(words)\n",
    "    # Sort words by frequency in descending order\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x:x[1], reverse=True)\n",
    "    # Create a dictionary that maps each word to a unique index\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "    # Add special token '<pad>' at index 0 for padding purposes\n",
    "    word2idx['<pad>'] = 0\n",
    "    # Add special token '<unk>' at index 1 to represent unknown words\n",
    "    word2idx['<unk>'] = 1\n",
    "    # Create a reverse dictionary to map indexes back to words\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7fa8738e",
   "metadata": {
    "id": "7fa8738e"
   },
   "outputs": [],
   "source": [
    "# Create English and Spanish vocabularies from respective sentence lists\n",
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "\n",
    "# Get vocabulary sizes for English and Spanish\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "spa_vocab_size = len(spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79d6b633",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "79d6b633",
    "outputId": "a9a3629e-2c02-42bc-9cb1-107b7aff42e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27649 46928\n"
     ]
    }
   ],
   "source": [
    "# Print vocabulary sizes to confirm\n",
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9082d83",
   "metadata": {
    "id": "c9082d83"
   },
   "source": [
    "### Custom Dataset class for English-Spanish sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e564017c",
   "metadata": {
    "id": "e564017c"
   },
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for handling English-Spanish sentence pairs.\n",
    "\n",
    "    This dataset takes pairs of sentences in English and Spanish, along with\n",
    "    respective vocabularies, and converts each sentence into a list of token\n",
    "    indexes according to the provided vocabularies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    eng_sentences : list of str\n",
    "        List of sentences in English to be used as input data.\n",
    "    spa_sentences : list of str\n",
    "        List of sentences in Spanish to be used as target data.\n",
    "    eng_word2idx : dict\n",
    "        Dictionary mapping English words to unique integer indexes.\n",
    "    spa_word2idx : dict\n",
    "        Dictionary mapping Spanish words to unique integer indexes.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __len__():\n",
    "        Returns the number of sentence pairs in the dataset.\n",
    "\n",
    "    __getitem__(idx):\n",
    "        Retrieves the token index tensors for the English and Spanish sentences\n",
    "        at the specified index.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (torch.Tensor, torch.Tensor)\n",
    "        A tuple containing:\n",
    "            - A tensor with token indexes for the English sentence.\n",
    "            - A tensor with token indexes for the Spanish sentence.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> eng_sentences = [\"hello\", \"how are you\"]\n",
    "    >>> spa_sentences = [\"hola\", \"como estas\"]\n",
    "    >>> dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "    >>> len(dataset)\n",
    "    2\n",
    "    >>> eng_tensor, spa_tensor = dataset[0]\n",
    "    >>> eng_tensor\n",
    "    tensor([...])\n",
    "    >>> spa_tensor\n",
    "    tensor([...])\n",
    "    \"\"\"\n",
    "    def __init__(self, eng_sentences, spa_sentences, eng_word2idx, spa_word2idx):\n",
    "        self.eng_sentences = eng_sentences  # List of English sentences\n",
    "        self.spa_sentences = spa_sentences  # List of Spanish sentences\n",
    "        self.eng_word2idx = eng_word2idx    # Vocabulary mapping for English\n",
    "        self.spa_word2idx = spa_word2idx    # Vocabulary mapping for Spanish\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of sentence pairs in the dataset\n",
    "        return len(self.eng_sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get token index tensors for the English and Spanish sentences at the specified index.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : int\n",
    "            The index of the sentence pair to retrieve.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of (torch.Tensor, torch.Tensor)\n",
    "            - eng_idxs : torch.Tensor\n",
    "                A tensor of token indexes representing the English sentence.\n",
    "            - spa_idxs : torch.Tensor\n",
    "                A tensor of token indexes representing the Spanish sentence.\n",
    "        \"\"\"\n",
    "        # Get the English and Spanish sentences at the specified index\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "        # Convert English sentence to list of token indexes, using <unk> for unknown words\n",
    "        eng_idxs = [self.eng_word2idx.get(word, self.eng_word2idx['<unk>']) for word in eng_sentence.split()]\n",
    "        # Convert Spanish sentence to list of token indexes, using <unk> for unknown words\n",
    "        spa_idxs = [self.spa_word2idx.get(word, self.spa_word2idx['<unk>']) for word in spa_sentence.split()]\n",
    "\n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826706a",
   "metadata": {
    "id": "8826706a"
   },
   "source": [
    "### Custom Collate function to prepare batches with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b579577b",
   "metadata": {
    "id": "b579577b"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for preparing batches of English-Spanish sentence pairs with padding.\n",
    "\n",
    "    This function processes a batch of token index tensors for English and Spanish sentences,\n",
    "    truncates them to a maximum sequence length, and applies padding so that all sequences\n",
    "    in the batch have the same length.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : list of tuple of (torch.Tensor, torch.Tensor)\n",
    "        A list of tuples, where each tuple contains:\n",
    "            - eng_tensor : torch.Tensor\n",
    "                A tensor of token indexes representing the English sentence.\n",
    "            - spa_tensor : torch.Tensor\n",
    "                A tensor of token indexes representing the Spanish sentence.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple of (torch.Tensor, torch.Tensor)\n",
    "        A tuple containing:\n",
    "            - eng_batch : torch.Tensor\n",
    "                A padded tensor of token indexes for the English sentences, with shape\n",
    "                (batch_size, padded_seq_len).\n",
    "            - spa_batch : torch.Tensor\n",
    "                A padded tensor of token indexes for the Spanish sentences, with shape\n",
    "                (batch_size, padded_seq_len).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> from torch.utils.data import DataLoader\n",
    "    >>> dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "    >>> dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn)\n",
    "    >>> for eng_batch, spa_batch in dataloader:\n",
    "    >>>     print(eng_batch.shape)\n",
    "    >>>     print(spa_batch.shape)\n",
    "    \"\"\"\n",
    "    # Separate English and Spanish sentences in the batch\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    # Truncate sentences to MAX_SEQ_LEN if necessary\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "    # Pad English sentences in the batch to the same length with padding value 0\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(eng_batch, batch_first=True, padding_value=0)\n",
    "    # Pad Spanish sentences in the batch to the same length with padding value 0\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(spa_batch, batch_first=True, padding_value=0)\n",
    "    return eng_batch, spa_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493caafb",
   "metadata": {
    "id": "493caafb"
   },
   "source": [
    "### Traning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8d514b7c",
   "metadata": {
    "id": "8d514b7c"
   },
   "outputs": [],
   "source": [
    "# Define the training function for the model\n",
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    \"\"\"\n",
    "    Train the given model on English-Spanish sentence pairs over a specified number of epochs.\n",
    "\n",
    "    This function performs training on the provided model using teacher forcing. For each epoch,\n",
    "    it iterates through batches from the dataloader, computes the loss, and updates model parameters\n",
    "    to minimize the loss. The function also displays the average loss for each epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The model to be trained, typically a neural network for sequence-to-sequence tasks.\n",
    "    dataloader : torch.utils.data.DataLoader\n",
    "        DataLoader object that provides batches of English and Spanish sentence pairs.\n",
    "    loss_function : torch.nn.Module\n",
    "        Loss function to calculate the error between model predictions and target output.\n",
    "    optimiser : torch.optim.Optimizer\n",
    "        Optimizer to update model parameters based on gradients computed from the loss.\n",
    "    epochs : int\n",
    "        The number of complete passes through the training dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    for epoch in range(epochs):  # Loop over each epoch\n",
    "        total_loss = 0  # Track cumulative loss for the epoch\n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            eng_batch = eng_batch.to(device)  # Move English batch to device\n",
    "            spa_batch = spa_batch.to(device)  # Move Spanish batch to device\n",
    "            # Prepare target input and output for the decoder\n",
    "            target_input = spa_batch[:, :-1]  # Shifted target input for teacher forcing\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)  # Shifted target output for prediction\n",
    "            # Reset gradients to avoid accumulation\n",
    "            optimiser.zero_grad()\n",
    "            # Forward pass through the model\n",
    "            output = model(eng_batch, target_input)\n",
    "            output = output.view(-1, output.size(-1))  # Flatten output for loss calculation\n",
    "            # Calculate loss between model output and target output\n",
    "            loss = loss_function(output, target_output)\n",
    "            # Backpropagate gradients\n",
    "            loss.backward()\n",
    "            # Update model parameters\n",
    "            optimiser.step()\n",
    "            # Accumulate loss for this batch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Calculate average loss for the epoch and print it\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f'Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13ceac7",
   "metadata": {
    "id": "d13ceac7"
   },
   "source": [
    "### Creating Instances and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2379ea72",
   "metadata": {
    "id": "2379ea72"
   },
   "outputs": [],
   "source": [
    "# Set batch size for training\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create dataset instance using English and Spanish sentences and their vocabularies\n",
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)\n",
    "\n",
    "# Initialize DataLoader to shuffle and batch the dataset, with custom collation function for padding\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e08eef6a",
   "metadata": {
    "id": "e08eef6a"
   },
   "outputs": [],
   "source": [
    "# Define the Transformer model with specified dimensions, number of layers, vocab sizes, and dropout\n",
    "model = Transformer(d_model=512, num_heads=8, d_ff=2048, num_layers=6,\n",
    "                    input_vocab_size=eng_vocab_size, target_vocab_size=spa_vocab_size,\n",
    "                    max_len=MAX_SEQ_LEN, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a1181a12",
   "metadata": {
    "id": "a1181a12"
   },
   "outputs": [],
   "source": [
    "# Move the model to the specified device (GPU if available)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define the loss function with padding token (<pad>) index ignored in the calculations\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Use the Adam optimizer with a small learning rate for stable training\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f5718",
   "metadata": {
    "id": "5c1f5718"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "14e265e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14e265e9",
    "outputId": "cd424346-5f30-40cf-c9f3-0ca4eec25a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 3.6027\n",
      "Epoch: 1/10, Loss: 2.2008\n",
      "Epoch: 2/10, Loss: 1.7015\n",
      "Epoch: 3/10, Loss: 1.3760\n",
      "Epoch: 4/10, Loss: 1.1248\n",
      "Epoch: 5/10, Loss: 0.9226\n",
      "Epoch: 6/10, Loss: 0.7586\n",
      "Epoch: 7/10, Loss: 0.6300\n",
      "Epoch: 8/10, Loss: 0.5348\n",
      "Epoch: 9/10, Loss: 0.4669\n"
     ]
    }
   ],
   "source": [
    "# Train the model using the specified dataloader, loss function, and optimizer for 10 epochs\n",
    "train(model, dataloader, loss_function, optimiser, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ttZ1XvumpzNe",
   "metadata": {
    "id": "ttZ1XvumpzNe"
   },
   "outputs": [],
   "source": [
    "# Save model for future use\n",
    "torch.save(model.state_dict(), './transformer_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d1f130",
   "metadata": {
    "id": "01d1f130"
   },
   "source": [
    "### Processing results functions for translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50740746",
   "metadata": {
    "code_folding": [],
    "id": "50740746"
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word2idx):\n",
    "    \"\"\"\n",
    "    Convert a sentence into a list of token indices based on the given vocabulary.\n",
    "\n",
    "    Each word in the sentence is mapped to its corresponding index in the vocabulary. If a word is not\n",
    "    found in the vocabulary, it is replaced with the index for the unknown token ('<unk>').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sentence : str\n",
    "        The sentence to be converted into indices.\n",
    "    word2idx : dict\n",
    "        A dictionary mapping words to their corresponding indices in the vocabulary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of int\n",
    "        A list of indices representing the sentence.\n",
    "    \"\"\"\n",
    "    return [word2idx.get(word, word2idx['<unk>']) for word in sentence.split()]\n",
    "\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    \"\"\"\n",
    "    Convert a list of token indices back into a sentence using the reverse vocabulary.\n",
    "\n",
    "    Each index is mapped back to its corresponding word in the vocabulary. The padding token ('<pad>')\n",
    "    is excluded from the output sentence.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    indices : list of int\n",
    "        The list of token indices to be converted into a sentence.\n",
    "    idx2word : dict\n",
    "        A dictionary mapping indices back to their corresponding words in the vocabulary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The reconstructed sentence as a string.\n",
    "    \"\"\"\n",
    "    return ' '.join([idx2word[idx] for idx in indices if idx in idx2word and idx2word[idx] != '<pad>'])\n",
    "\n",
    "def translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    \"\"\"\n",
    "    Translate a given English sentence into Spanish using a pre-trained model.\n",
    "\n",
    "    The function preprocesses the input sentence, converts it to token indices, and feeds it through the model\n",
    "    in evaluation mode. The model generates the Spanish translation until it reaches either the maximum sequence\n",
    "    length or the end-of-sentence token ('<eos>').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained translation model.\n",
    "    sentence : str\n",
    "        The input English sentence to translate.\n",
    "    eng_word2idx : dict\n",
    "        Vocabulary dictionary mapping English words to indices.\n",
    "    spa_idx2word : dict\n",
    "        Reverse vocabulary dictionary mapping Spanish indices back to words.\n",
    "    max_len : int, optional\n",
    "        The maximum length for the generated translation (default is MAX_SEQ_LEN).\n",
    "    device : str, optional\n",
    "        The device on which the model is run, e.g., 'cpu' or 'cuda' (default is 'cpu').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The translated Spanish sentence.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode for inference\n",
    "    sentence = preprocess_sentence(sentence)  # Preprocess sentence (e.g., lowercase, clean)\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)  # Convert input sentence to indices\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Initialize the target tensor with the start-of-sentence (<sos>) token\n",
    "    tgt_indices = [spa_word2idx['<sos>']]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Generate tokens until reaching max length or end-of-sentence (<eos>) token\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_tensor, tgt_tensor)  # Pass source and target through the model\n",
    "            output = output.squeeze(0)  # Remove batch dimension for easier processing\n",
    "            next_token = output.argmax(dim=-1)[-1].item()  # Get index of the most likely next token\n",
    "            tgt_indices.append(next_token)  # Append this token to target indices\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)  # Update target tensor for next step\n",
    "            if next_token == spa_word2idx['<eos>']:  # Stop if <eos> token is generated\n",
    "                break\n",
    "\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)  # Convert indices to sentence for final output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62d7c6d",
   "metadata": {
    "id": "b62d7c6d"
   },
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2c0db72",
   "metadata": {
    "code_folding": [
     15
    ],
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2c0db72",
    "outputId": "09c5dc41-1847-428b-b04b-c11b36dcf01c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: I am not a robot.\n",
      "Translation: <sos> no soy un robot <eos>\n",
      "\n",
      "Input sentence: I like to pet my dog\n",
      "Translation: <sos> me gusta mi perro de mascota <eos>\n",
      "\n",
      "Input sentence: I can learn artificial intelligence.\n",
      "Translation: <sos> puedo aprender inteligencia artificial <eos>\n",
      "\n",
      "Input sentence: The dinner is on the table\n",
      "Translation: <sos> la cena esta sobre la mesa <eos>\n",
      "\n",
      "Input sentence: I am a human.\n",
      "Translation: <sos> yo soy un humano <eos>\n",
      "\n",
      "Input sentence: Hello friend.\n",
      "Translation: <sos> hola amigo <eos>\n",
      "\n",
      "Input sentence: Do you like artificial intelligence?\n",
      "Translation: <sos> te gusta la inteligencia artificial <eos>\n",
      "\n",
      "Input sentence: It's a nice evening.\n",
      "Translation: <sos> es una buena noche <eos>\n",
      "\n",
      "Input sentence: I did my homework.\n",
      "Translation: <sos> hice mis deberes <eos>\n",
      "\n",
      "Input sentence: We should go out someday.\n",
      "Translation: <sos> deberiamos salir algun dia <eos>\n",
      "\n",
      "Input sentence: I will go to my home!.\n",
      "Translation: <sos> me ire a mi casa <eos>\n",
      "\n",
      "Input sentence: My car is blue\n",
      "Translation: <sos> mi coche es azul <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to evaluate translations on a list of English sentences\n",
    "def evaluate_translations(model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate translations for a list of English sentences using a pre-trained model.\n",
    "\n",
    "    This function iterates over a list of English sentences, translates each sentence to Spanish\n",
    "    using the specified model, and prints both the original sentence and its translated output.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : torch.nn.Module\n",
    "        The trained translation model.\n",
    "    sentences : list of str\n",
    "        A list of English sentences to be translated.\n",
    "    eng_word2idx : dict\n",
    "        Vocabulary dictionary mapping English words to indices.\n",
    "    spa_idx2word : dict\n",
    "        Reverse vocabulary dictionary mapping Spanish indices back to words.\n",
    "    max_len : int, optional\n",
    "        The maximum length for the generated translation (default is MAX_SEQ_LEN).\n",
    "    device : str, optional\n",
    "        The device on which the model is run, e.g., 'cpu' or 'cuda' (default is 'cpu').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        This function does not return any value. It prints each input sentence and its translation to the console.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        # Translate each sentence using the translate_sentence function\n",
    "        translation = translate_sentence(model, sentence, eng_word2idx, spa_idx2word, max_len, device)\n",
    "        # Print original sentence and its translation for review\n",
    "        print(f'Input sentence: {sentence}')\n",
    "        print(f'Translation: {translation}')\n",
    "        print()\n",
    "\n",
    "# Example sentences to test the translation model\n",
    "test_sentences = [\n",
    "    \"I am not a robot.\",\n",
    "    \"I like to pet my dog\",\n",
    "    \"I can learn artificial intelligence.\",\n",
    "    \"The dinner is on the table\",\n",
    "    \"I am a human.\",\n",
    "    \"Hello friend.\",\n",
    "    \"Do you like artificial intelligence?\",\n",
    "    \"It's a nice evening.\",\n",
    "    \"I did my homework.\",\n",
    "    \"We should go out someday.\",\n",
    "    \"I will go to my home!.\",\n",
    "    \"My car is blue\"\n",
    "]\n",
    "\n",
    "# Move model to available device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Run evaluation to print translations of test sentences\n",
    "evaluate_translations(model, test_sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dtuoz0lrxwU",
   "metadata": {
    "id": "8dtuoz0lrxwU"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we demonstrate how we can use a transformer model to build an English to Spanish translator. The input data to train the model is a dataset found on the internet comprising 266,338 English to Spanish sentences.\n",
    "\n",
    "The training processes in this notebook map all the data to train and validate the operations of the transformer model. Each part of the transformer results in an ingenious engineering implementation starting with the sine operation, the encoder block using a multi-head attention scheme providing useful parameters Q, K, and V running in a parallel procedure. Then the decoder block using the information provided by the encoder block to compare key results.\n",
    "\n",
    "Even with the lack of a full translator for both languages, and even when training is done only in a few epochs, it is quite interesting how well the translation performs in the test and demonstrate how powerful a transformer can be."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
